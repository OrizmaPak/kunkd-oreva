"use strict";

Object.defineProperty(exports, "__esModule", {
  value: true
});
exports.Events = exports.Controller = void 0;
var _utils = require("./utils.cjs");
var Events = exports.Events = /*#__PURE__*/function (Events) {
  Events["BOUNDARY"] = "boundary";
  Events["END"] = "end";
  Events["ERROR"] = "error";
  Events["PAUSED"] = "paused";
  Events["PITCH"] = "pitch";
  Events["PLAYING"] = "playing";
  Events["RATE"] = "rate";
  Events["READY"] = "ready";
  Events["VOLUME"] = "volume";
  return Events;
}(Events || {});
class Controller extends EventTarget {
  #target;
  #synthesizer;
  #dispatchBoundaries = true;
  #fetchAudioData = async () => ({
    audio: '',
    marks: []
  });
  #marks = [];
  #text = '';
  #lang = '';
  #aborter = new AbortController();
  #initialized = false;
  constructor(options) {
    super();
    this.#lang = options?.lang ?? this.#lang;
    this.#synthesizer = window.speechSynthesis;
    this.#target = new SpeechSynthesisUtterance(this.#text);
    this.#dispatchBoundaries = options?.dispatchBoundaries ?? this.#dispatchBoundaries;
    if (options?.fetchAudioData) {
      this.#target = this.#synthesizer = new Audio();
      this.#fetchAudioData = options.fetchAudioData;
    } else {
      this.#initWebSpeechVoice(options?.voice);
      if (window.speechSynthesis) {
        window.speechSynthesis.onvoiceschanged = () => {
          this.#initWebSpeechVoice(options?.voice);
        };
      }
    }
  }
  #initWebSpeechVoice(voice) {
    if (this.#target instanceof SpeechSynthesisUtterance) {
      let voices = window.speechSynthesis.getVoices();
      if (voice) {
        this.#target.voice = voice;
      }
      if (this.#lang) {
        voices = voices.filter(voice => voice.lang === this.#lang);
        this.#target.voice = voices[0] ?? null;
        if (voice && voice.lang === this.#lang) {
          this.#target.voice = voice;
        }
      }
    }
  }
  async #attachAudioSource() {
    if (this.#synthesizer instanceof HTMLAudioElement) {
      let data = null;
      try {
        data = await this.#fetchAudioData(this.#text);
      } catch (err) {
        if (err instanceof Error) {
          this.#dispatchError(err.message);
        }
      } finally {
        if (data?.audio) {
          this.#synthesizer.src = data.audio;
          this.#marks = data.marks ?? this.#marks;
        }
      }
    }
  }
  #dispatchEnd(evt) {
    this.dispatchEvent(new CustomEvent(Events.END, {
      detail: evt
    }));
  }
  #dispatchError(msg) {
    this.dispatchEvent(new CustomEvent(Events.ERROR, {
      detail: msg
    }));
  }
  #dispatchReady() {
    this.dispatchEvent(new Event(Events.READY));
  }
  #dispatchPlaying(evt) {
    this.dispatchEvent(new CustomEvent(Events.PLAYING, {
      detail: evt
    }));
  }
  #dispatchPaused(evt) {
    this.dispatchEvent(new CustomEvent(Events.PAUSED, {
      detail: evt
    }));
  }
  #dispatchBoundary(evt, boundary) {
    this.dispatchEvent(new CustomEvent(Events.BOUNDARY, {
      detail: {
        evt,
        boundary
      }
    }));
  }
  #dispatchVolume(volume) {
    this.dispatchEvent(new CustomEvent(Events.VOLUME, {
      detail: volume
    }));
  }
  #dispatchRate(rate) {
    this.dispatchEvent(new CustomEvent(Events.RATE, {
      detail: rate
    }));
  }
  #dispatchPitch(pitch) {
    this.dispatchEvent(new CustomEvent(Events.PITCH, {
      detail: pitch
    }));
  }
  async #playHtmlAudio() {
    const audio = this.#synthesizer;
    try {
      await audio.play();
    } catch (err) {
      if (err instanceof Error) {
        this.#dispatchError(err.message);
      }
    }
  }
  #getPollySpeechMarkForAudioTime(time) {
    const length = this.#marks.length;
    let bestMatch = this.#marks[0];
    let found = false;
    let i = 1;
    while (i < length && !found) {
      if (this.#marks[i].time <= time) {
        bestMatch = this.#marks[i];
      } else {
        found = true;
      }
      i++;
    }
    return bestMatch;
  }

  /**
   * Not all browsers return `evt.charLength` on SpeechSynthesisUtterance `boundary` events.
   */
  #getBoundaryWordCharLength(startIndex) {
    const match = this.#text.substring(startIndex).match(/.+?\b/);
    return match ? match[0].length : 0;
  }
  #clamp(value) {
    let min = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;
    let max = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;
    return Math.max(min, Math.min(value, max));
  }

  /**
   * Removes registered listeners and creates new abort controller.
   */
  #recycle() {
    this.#aborter.abort();
    this.#aborter = new AbortController();
    return this.#aborter.signal;
  }
  #utteranceInit() {
    if (this.#target instanceof SpeechSynthesisUtterance) {
      const signal = this.#recycle();
      this.#target.addEventListener('end', this.#dispatchEnd.bind(this), {
        signal
      });
      this.#target.addEventListener('start', this.#dispatchPlaying.bind(this), {
        signal
      });
      this.#target.addEventListener('resume', this.#dispatchPlaying.bind(this), {
        signal
      });
      this.#target.addEventListener('pause', this.#dispatchPaused.bind(this), {
        signal
      });
      this.#target.addEventListener('error', evt => {
        this.#dispatchError(evt.error);
      }, {
        signal
      });
      if (this.#lang) {
        this.#target.lang = this.#lang;
      }
      if (this.#dispatchBoundaries) {
        this.#target.addEventListener('boundary', evt => {
          const {
            charIndex: startChar
          } = evt;
          const charLength = evt.charLength ?? this.#getBoundaryWordCharLength(startChar);
          const endChar = startChar + charLength;
          const word = this.#text.substring(startChar, endChar);
          if (word && !(0, _utils.isPunctuation)(word)) {
            this.#dispatchBoundary(evt, {
              word,
              startChar,
              endChar
            });
          }
        }, {
          signal
        });
      }
      this.#dispatchReady();
    }
  }
  async #htmlAudioInit() {
    if (this.#target instanceof HTMLAudioElement) {
      const target = this.#target;
      this.#target.addEventListener('canplay', this.#dispatchReady.bind(this), {
        once: true
      });
      this.#target.addEventListener('playing', this.#dispatchPlaying.bind(this));
      this.#target.addEventListener('pause', this.#dispatchPaused.bind(this));
      this.#target.addEventListener('ended', this.#dispatchEnd.bind(this));
      this.#target.addEventListener('error', () => {
        const error = target.error;
        this.#dispatchError(error?.message);
      });
      if (this.#dispatchBoundaries) {
        this.#target.addEventListener('timeupdate', evt => {
          // Polly Speech Marks use milliseconds
          const currentTime = target.currentTime * 1000;
          const mark = this.#getPollySpeechMarkForAudioTime(currentTime);
          if (mark && !this.paused) {
            this.#dispatchBoundary(evt, {
              word: mark.value,
              startChar: mark.start,
              endChar: mark.end
            });
          }
        });
      }
      await this.#attachAudioSource();
    }
  }
  get synthesizer() {
    return this.#synthesizer;
  }
  get target() {
    return this.#target;
  }
  set text(value) {
    this.#text = value;
    if (this.#target instanceof SpeechSynthesisUtterance) {
      this.#target.text = value;
    }
  }
  get paused() {
    return this.#synthesizer.paused;
  }
  get rate() {
    if (this.#synthesizer instanceof HTMLAudioElement) {
      return this.#synthesizer.playbackRate;
    }
    return this.#target.rate;
  }
  set rate(value) {
    const clamped = this.#clamp(parseFloat(value.toPrecision(3)), 0.1, 10);
    if (!Number.isNaN(clamped)) {
      this.#dispatchRate(clamped);
      if (this.#synthesizer instanceof HTMLAudioElement) {
        this.#synthesizer.defaultPlaybackRate = clamped;
        this.#synthesizer.playbackRate = clamped;
      }
      if (this.#target instanceof SpeechSynthesisUtterance) {
        this.#target.rate = clamped;
      }
    }
  }
  get pitch() {
    if (this.#target instanceof SpeechSynthesisUtterance) {
      return this.#target.pitch;
    }

    // Not supported by HTMLAudioElement
    return -1;
  }
  set pitch(value) {
    if (this.#target instanceof SpeechSynthesisUtterance) {
      const clamped = this.#clamp(parseFloat(value.toPrecision(2)), 0, 2);
      if (!Number.isNaN(clamped)) {
        this.#dispatchPitch(clamped);
        this.#target.pitch = clamped;
      }
    }
  }
  get volumeMin() {
    return 0;
  }
  get volumeMax() {
    return 1;
  }
  get volume() {
    return this.#target.volume;
  }
  set volume(value) {
    const clamped = this.#clamp(parseFloat(value.toPrecision(2)), this.volumeMin, this.volumeMax);
    if (!Number.isNaN(clamped)) {
      this.#dispatchVolume(clamped);
      this.#target.volume = clamped;
    }
  }
  get preservesPitch() {
    if (this.#synthesizer instanceof HTMLAudioElement) {
      return this.#synthesizer.preservesPitch;
    }
    return false;
  }
  set preservesPitch(value) {
    /**
     * `preservesPitch` requires vendor-prefix on some browsers (Safari).
     * @see https://github.com/microsoft/TypeScript-DOM-lib-generator/pull/1300
     */
    if (this.#synthesizer instanceof HTMLAudioElement) {
      // eslint-disable-next-line no-extra-semi
      ;
      this.#synthesizer.preservesPitch = value;
    }
  }
  get lang() {
    return this.#lang;
  }
  set lang(value) {
    if (this.#target instanceof SpeechSynthesisUtterance) {
      this.#lang = value;
      this.#target.lang = value;
      this.#target.voice = null;
      this.#initWebSpeechVoice();
    }
  }

  /**
   * Allows listeners for controller events to be registered
   * before instances start firing events related to underlying API's,
   * for instance in a useEffect block.
   *
   * Run it as async to allow for the fetchAudioData call to be awaited.
   */
  async init() {
    if (!this.#initialized) {
      if (this.#target instanceof SpeechSynthesisUtterance) {
        this.#utteranceInit();
      }
      if (this.#target instanceof HTMLAudioElement) {
        await this.#htmlAudioInit();
      }
      this.#initialized = true;
    }
  }
  async play() {
    if (this.#synthesizer instanceof HTMLAudioElement) {
      await this.#playHtmlAudio();
    } else {
      this.#synthesizer.speak(this.#target);
    }
  }
  pause() {
    this.#synthesizer.pause();
  }
  async resume() {
    if (this.#synthesizer instanceof HTMLAudioElement) {
      await this.#playHtmlAudio();
    } else {
      this.#synthesizer.resume();
    }
  }
  async replay() {
    if (this.#synthesizer instanceof HTMLAudioElement) {
      this.#synthesizer.load();
      await this.#playHtmlAudio();
    } else {
      // Take out of any paused state
      this.#synthesizer.resume();
      // Drop all utterances in the queue
      this.#synthesizer.cancel();
      // Start speaking from the beginning
      this.#synthesizer.speak(this.#target);
    }
  }
  cancel() {
    if (this.#synthesizer instanceof HTMLAudioElement) {
      this.#synthesizer.load();
    } else {
      this.#synthesizer.cancel();
    }
  }
  mute() {
    this.volume = 0;

    /**
     * There is no way to effectively mute an ongoing utterance for SpeechSynthesis.
     * If there is currently an utterance being spoken, replay to activate the muting instantly.
     */
    if (!(this.#synthesizer instanceof HTMLAudioElement) && !this.paused && this.#synthesizer.speaking) {
      this.replay();
    }
  }
  unmute(volume) {
    this.volume = volume ?? 1;

    /**
     * Same as muting, for SpeechSynthesis have to replay to activate the volume change instantly.
     */
    if (!(this.#synthesizer instanceof HTMLAudioElement) && !this.paused && this.#synthesizer.speaking) {
      this.replay();
    }
  }
}
exports.Controller = Controller;